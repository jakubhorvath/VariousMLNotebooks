{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "YaPbEXlHvd_C",
      "metadata": {
        "id": "YaPbEXlHvd_C",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# Second Term Project: CQADupStack Collection\n",
        "\n",
        "The *CQADupStack* is \"[a] Benchmark Data Set for Community Question-Answering Research\" [1] that is a part of the [*Benchmarking Information Retrieval (BEIR)*](https://github.com/beir-cellar/beir) collection.\n",
        "\n",
        "CQADupStack contains data from 12 different [*Stackexchange*](https://stackexchange.com/) subforums based on the data dump released on September 26, 2014.\n",
        "\n",
        "Your tasks, reviewed by your colleagues and the course instructors, are the following:\n",
        "\n",
        "\n",
        "\n",
        "1. *Implement a ranked retrieval system*, [1, Chapter 6] which will produce a list of documents from the CQADupStack collection in a descending order of relevance to a query from the CQADupStack collection.\n",
        "2. *Document your code* in accordance with [PEP 257](https://www.python.org/dev/peps/pep-0257/), ideally using [the NumPy style guide](https://numpydoc.readthedocs.io/en/latest/format.html#docstring-standard) as seen in the code from exercises.\n",
        "   *Stick to a consistent coding style* in accordance with [PEP 8](https://www.python.org/dev/peps/pep-0008/).\n",
        "3. *Reach at least 25% mean average precision at 10* [1, Section 8.4] with your system on the CQADupStack collection.\n",
        "4. _[Upload an .ipynb file](https://is.muni.cz/help/komunikace/spravcesouboru#k_ss_1) with this Jupyter notebook to the homework vault in IS MU._ You MAY also include a brief description of your information retrieval system and a link to an external service such as [Google Colaboratory](https://colab.research.google.com/), [DeepNote](https://deepnote.com/), or [JupyterHub](https://iirhub.cloud.e-infra.cz/).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "[1] Hoogeveen, Doris and Verspoor, Karin M. and Baldwin, Timothy. [*CQADupStack: A Benchmark Data Set for Community Question-Answering Research*](https://dl.acm.org/doi/10.1145/2838931.2838934). ACM, 2015."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "u6zDHKMAvvTt",
      "metadata": {
        "id": "u6zDHKMAvvTt",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Import the utility tools from the git repository.\n",
        "\n",
        "First, we will install [our library](https://github.com/MIR-MU/pv211-utils).\n",
        "\n",
        "It may be necessary to restart the runtime to get the installed packages to work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c23a317",
      "metadata": {
        "id": "0c23a317"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "! pip install git+https://github.com/MIR-MU/pv211-utils.git"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "I0_jZfSIwfcv",
      "metadata": {
        "id": "I0_jZfSIwfcv",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Define the necessary classes\n",
        "\n",
        "These will eventually represent the Queries, Documents and Relevance Judgements from the CQADupStack collection.\n",
        "\n",
        "Query and Document consist only of their IDs and bodies.\n",
        "Judgements are also just a Set of Tuples that represent pairs of relevant Document-Query combinations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3f3f1a8-2a5b-4d3f-9e68-175909efe906",
      "metadata": {
        "id": "d3f3f1a8-2a5b-4d3f-9e68-175909efe906",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "from pv211_utils.beir.entities import BeirDocumentBase, BeirQueryBase, BeirJudgementBase\n",
        "from typing import Set\n",
        "\n",
        "\n",
        "class Query(BeirQueryBase):\n",
        "    \"\"\"\n",
        "    A processed query form the Beir collection.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    query_id : int\n",
        "        The number\n",
        "    body : str\n",
        "        Text of a query\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, query_id: int, body: str):\n",
        "        super().__init__(query_id, body)\n",
        "\n",
        "    def __str__(self):\n",
        "        return self.body\n",
        "\n",
        "\n",
        "class Document(BeirDocumentBase):\n",
        "    \"\"\"\n",
        "    A processed document form the Beir collection.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    document_id : str\n",
        "        A unique identifier of the document.\n",
        "    body : str\n",
        "        The text of the document.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, document_id: str, body: str):\n",
        "        super().__init__(document_id, body)\n",
        "\n",
        "    def __str__(self):\n",
        "        return self.body\n",
        "\n",
        "\n",
        "BeirJudgements = Set[BeirJudgementBase]\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "5f8f6e32",
      "metadata": {
        "id": "5f8f6e32",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Loading the datasets\n",
        "### CQADupStack contains 12 datasets that will be loaded and merged:\n",
        "- Android\n",
        "- English\n",
        "- Gaming\n",
        "- GIS\n",
        "- Mathematica\n",
        "- Physics\n",
        "- Programmers\n",
        "- Stats\n",
        "- TeX\n",
        "- Unix\n",
        "- Webmasters\n",
        "- WordPress\n",
        "\n",
        "For more details: <a href=http://nlp.cis.unimelb.edu.au/resources/cqadupstack/>CQADupStack site</a>.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1f2a62d",
      "metadata": {
        "id": "e1f2a62d"
      },
      "outputs": [],
      "source": [
        "from pv211_utils.datasets import CQADupStackDataset \n",
        "\n",
        "data = CQADupStackDataset(validation_split_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8649d766",
      "metadata": {
        "id": "8649d766"
      },
      "outputs": [],
      "source": [
        "documents = data.load_documents(document_class=Document)\n",
        "\n",
        "train_queries = data.load_train_queries(query_class=Query)\n",
        "train_judgements = data.load_train_judgements()\n",
        "\n",
        "validation_queries = data.load_validation_queries(query_class=Query)\n",
        "validation_judgements = data.load_validation_judgements()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "3fdd10d1",
      "metadata": {
        "id": "3fdd10d1"
      },
      "source": [
        "## Implementation of information retrieval system\n",
        "\n",
        "Here we will define our IR system. If you want to use your own class it must define a method name `search` that takes a query and returns documents in descending order of relevance to the query.\n",
        "\n",
        "This example returns documents in a decreasing order according to\n",
        "a [*Okapi BestMatch25+*](https://en.wikipedia.org/wiki/Okapi_BM25#Modifications) similarity score between the documents and the given query.\n",
        "\n",
        "If you wish you might use [preprocessing](https://github.com/MIR-MU/pv211-utils/tree/main/pv211_utils/preprocessing) or [ensemble](https://github.com/MIR-MU/pv211-utils/blob/developer/pv211_utils/ensembles.py) techniques from our library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6856e1c-48f4-43df-83cf-f7123c9a9196",
      "metadata": {
        "id": "b6856e1c-48f4-43df-83cf-f7123c9a9196",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Move the processing to the cuda\n",
        "\n",
        "import torch\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcbf6c85-6547-430b-a440-568a34983dfb",
      "metadata": {
        "id": "bcbf6c85-6547-430b-a440-568a34983dfb"
      },
      "outputs": [],
      "source": [
        "from gensim.parsing.porter import PorterStemmer\n",
        "from gensim.utils import simple_preprocess\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from typing import Union\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "\n",
        "def preprocess(document):\n",
        "    \"\"\"The preprocessing of a given document, which removes stopwords and does the stemming.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        document : Document\n",
        "            The instance of Document class.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        tokens : List[str]\n",
        "            The list of tokens of a given document, retrieved by applying various preprocessing techniques.\n",
        "            \n",
        "    \"\"\"\n",
        "    stemmer = PorterStemmer()\n",
        "    gist_file = open(\"gist_stopwords.txt\", \"r\")\n",
        "    try:\n",
        "      content = gist_file.read()\n",
        "      stopwords = content.split(\",\")\n",
        "    finally:\n",
        "      gist_file.close()\n",
        "    tokens = [stemmer.stem(item) for item in word_tokenize(document) if not item in stopwords]\n",
        "\n",
        "    return \" \".join(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00f93303-d1ac-416f-8d6a-b8920f38987f",
      "metadata": {
        "id": "00f93303-d1ac-416f-8d6a-b8920f38987f",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from typing import Iterable\n",
        "import numpy as np\n",
        "\n",
        "from sentence_transformers import InputExample\n",
        "from sentence_transformers.losses import MultipleNegativesRankingLoss\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from tqdm import tqdm\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sentence_transformers.util import dot_score\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "\n",
        "class IRSystem():\n",
        "    \"\"\"\n",
        "    A system that returns documents ordered by decreasing cosine similarity, using the predictions of finetuned transformer model.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    document_embeddings: \n",
        "        The dictionary of the system.\n",
        "    transformer: SentenceTransformer\n",
        "        The indexed documents.\n",
        "    index_to_document: dict of (int, Document)\n",
        "        A mapping from indexed document numbers to documents.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # Generate index_to_document dictionary\n",
        "        index_to_document = dict(enumerate(documents.values()))\n",
        "        self.index_to_document = index_to_document\n",
        "        \n",
        "        # Download pretrained model from Huggingface\n",
        "        transformer = SentenceTransformer(\"all-mpnet-base-v2\").to(device)\n",
        "\n",
        "        # Define the train examples, cropped to the length of 512 due to input length limitation of transformer.\n",
        "        train_examples = [ InputExample(texts=[preprocess(judgement[0].body), \n",
        "                                                preprocess(judgement[1].body[:512])], \n",
        "                                        label=1.) for judgement in train_judgements\n",
        "                          ]\n",
        "\n",
        "        # Define your train dataset and the dataloader:\n",
        "        train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n",
        "\n",
        "        # Finally, pick the objective for training (i.e. loss):\n",
        "        train_loss = MultipleNegativesRankingLoss(transformer)\n",
        "\n",
        "        # Tune the model\n",
        "        transformer.fit(train_objectives=[(train_dataloader, train_loss)], epochs=1, warmup_steps=100)\n",
        "\n",
        "        # Generate embeddings from document bodies\n",
        "        document_bodies = [preprocess(document.body) for document in documents.values()]\n",
        "        document_embeddings = []\n",
        "        for index in tqdm(range(0, len(document_bodies), 1028), desc='Dataset'):\n",
        "            document_embeddings_b = transformer.encode(document_bodies[index:min(index+1200, len(document_bodies))])\n",
        "            document_embeddings.extend(document_embeddings_b)\n",
        "        \n",
        "        self.document_embeddings = document_embeddings\n",
        "        self.transformer = transformer\n",
        "\n",
        "    def search(self, query: Query) -> Iterable[Document]:\n",
        "        \"\"\"The ranked retrieval results for a query.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        query : Query\n",
        "            A query.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        iterable of Document\n",
        "            The ranked retrieval results for a query.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # Calculate list of similarities using the transformer model\n",
        "        query_vector_semantic = self.transformer.encode(preprocess(query.body))\n",
        "        similarities_semantic = [\n",
        "            (index, np.dot(query_vector_semantic, doc_vector))\n",
        "            for index, doc_vector in enumerate(self.document_embeddings)\n",
        "        ]\n",
        "\n",
        "        # Combine similarities to calculate final list\n",
        "        similarities = sorted((similarities_semantic), key=lambda item: item[1], reverse=True)\n",
        "\n",
        "        for (document_number, _) in similarities:\n",
        "            document = self.index_to_document[document_number]\n",
        "            yield document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e80203d",
      "metadata": {
        "id": "4e80203d"
      },
      "outputs": [],
      "source": [
        "from pv211_utils.systems import BM25PlusSystem, TfidfSystem\n",
        "from pv211_utils.preprocessing import SimpleDocPreprocessing\n",
        "\n",
        "system = IRSystem()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "EiuNlaI76EPe",
      "metadata": {
        "id": "EiuNlaI76EPe",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Evaluate the system on a given dataset\n",
        "\n",
        "We will evaluate the IR system using the [Mean Average Precision](https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Mean_average_precision) (MAP)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a297dbfb",
      "metadata": {
        "id": "a297dbfb",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from pv211_utils.beir.leaderboard import BeirLeaderboard\n",
        "from pv211_utils.beir.eval import BeirEvaluation\n",
        "\n",
        "submit_result = False\n",
        "author_name = \"\"\n",
        "\n",
        "test_queries = data.load_test_queries(Query)\n",
        "test_judgements = data.load_test_judgements()\n",
        "leaderboard = BeirLeaderboard()\n",
        "evaluation = BeirEvaluation(system, test_judgements, k=10, leaderboard=leaderboard, author_name=author_name)\n",
        "evaluation.evaluate(test_queries, submit_result)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "97b31d6e62de2216a05dd9342162045e53cee058ed98d00a361b193ba69cab9f"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
